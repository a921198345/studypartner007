#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AIå¢å¼ºçš„æ³•å¾‹æ–‡æ¡£å¤„ç†æ¨¡å—
é›†æˆDeepSeek APIï¼Œæä¾›æ™ºèƒ½ç»“æ„è¯†åˆ«å’Œåˆ†æ®µä¼˜åŒ–
"""

import re
import json
import requests
import os
from typing import List, Dict, Any, Optional
from text_segmentation import LegalDocumentSegmenter, LegalSegment

class AIEnhancedLegalProcessor:
    """AIå¢å¼ºçš„æ³•å¾‹æ–‡æ¡£å¤„ç†å™¨"""
    
    def __init__(self, api_key: str = None):
        self.base_segmenter = LegalDocumentSegmenter()
        self.api_key = api_key or os.getenv('DEEPSEEK_API_KEY')
        self.api_base_url = "https://api.deepseek.com/v1"
        
        if not self.api_key:
            print("âš ï¸  è­¦å‘Š: æœªè®¾ç½®DEEPSEEK_API_KEYï¼ŒAIå¢å¼ºåŠŸèƒ½å°†ä¸å¯ç”¨")
    
    def call_deepseek_api(self, prompt: str, model: str = "deepseek-chat") -> str:
        """è°ƒç”¨DeepSeek API"""
        if not self.api_key:
            raise ValueError("éœ€è¦è®¾ç½®DEEPSEEK_API_KEYæ‰èƒ½ä½¿ç”¨AIåŠŸèƒ½")
        
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        
        data = {
            "model": model,
            "messages": [
                {"role": "user", "content": prompt}
            ],
            "temperature": 0.1,  # è¾ƒä½æ¸©åº¦ç¡®ä¿ç»“æœä¸€è‡´æ€§
            "max_tokens": 1000
        }
        
        try:
            response = requests.post(
                f"{self.api_base_url}/chat/completions",
                headers=headers,
                json=data,
                timeout=30
            )
            response.raise_for_status()
            
            result = response.json()
            return result["choices"][0]["message"]["content"]
            
        except requests.exceptions.RequestException as e:
            print(f"âŒ APIè°ƒç”¨å¤±è´¥: {e}")
            return ""
    
    def ai_enhance_structure_recognition(self, text: str) -> Dict[str, Any]:
        """ä½¿ç”¨AIå¢å¼ºæ³•å¾‹æ–‡æ¡£ç»“æ„è¯†åˆ«"""
        prompt = f"""
è¯·åˆ†æä»¥ä¸‹æ³•å¾‹æ–‡æ¡£çš„ç»“æ„ï¼Œè¯†åˆ«å…¶å±‚æ¬¡å…³ç³»ï¼š

æ–‡æ¡£å†…å®¹ï¼š
{text[:2000]}  # åªå–å‰2000å­—ç¬¦é¿å…tokenè¿‡å¤š

è¯·è¿”å›JSONæ ¼å¼çš„ç»“æ„åˆ†æï¼ŒåŒ…æ‹¬ï¼š
1. æ–‡æ¡£ç±»å‹ï¼ˆæ³•å¾‹ã€æ³•è§„ã€æ¡ä¾‹ç­‰ï¼‰
2. ä¸»è¦ç»“æ„å±‚æ¬¡ï¼ˆç¼–ã€ç« ã€èŠ‚ã€æ¡ç­‰ï¼‰
3. å»ºè®®çš„åˆ†æ®µç­–ç•¥
4. ç‰¹æ®Šæ ¼å¼è¯´æ˜

è¿”å›æ ¼å¼ï¼š
{{
    "document_type": "æ³•å¾‹æ–‡æ¡£ç±»å‹",
    "structure_levels": ["ç¼–", "ç« ", "èŠ‚", "æ¡"],
    "segmentation_strategy": "å»ºè®®çš„åˆ†æ®µæ–¹å¼",
    "special_formats": ["ç‰¹æ®Šæ ¼å¼è¯´æ˜"]
}}
"""
        
        try:
            ai_response = self.call_deepseek_api(prompt)
            # å°è¯•è§£æJSONå“åº”
            if ai_response:
                # æå–JSONéƒ¨åˆ†
                json_match = re.search(r'\{.*\}', ai_response, re.DOTALL)
                if json_match:
                    return json.loads(json_match.group())
        except Exception as e:
            print(f"âš ï¸  AIç»“æ„åˆ†æå¤±è´¥: {e}")
        
        # è¿”å›é»˜è®¤ç»“æ„
        return {
            "document_type": "æ³•å¾‹æ–‡æ¡£",
            "structure_levels": ["ç¼–", "ç« ", "èŠ‚", "æ¡"],
            "segmentation_strategy": "æŒ‰æ¡æ–‡åˆ†æ®µ",
            "special_formats": []
        }
    
    def ai_optimize_segmentation(self, segments: List[LegalSegment]) -> List[LegalSegment]:
        """ä½¿ç”¨AIä¼˜åŒ–åˆ†æ®µè´¨é‡"""
        if not self.api_key:
            print("âš ï¸  è·³è¿‡AIä¼˜åŒ–ï¼ˆéœ€è¦APIå¯†é’¥ï¼‰")
            return segments
        
        optimized_segments = []
        
        for segment in segments:
            # å¯¹æ¯ä¸ªåˆ†æ®µè¿›è¡Œè´¨é‡æ£€æŸ¥
            if len(segment.content) > 100:  # åªå¯¹è¾ƒé•¿çš„åˆ†æ®µè¿›è¡Œä¼˜åŒ–
                optimized_segment = self._optimize_single_segment(segment)
                optimized_segments.append(optimized_segment)
            else:
                optimized_segments.append(segment)
        
        return optimized_segments
    
    def _optimize_single_segment(self, segment: LegalSegment) -> LegalSegment:
        """ä¼˜åŒ–å•ä¸ªåˆ†æ®µ"""
        prompt = f"""
è¯·åˆ†æä»¥ä¸‹æ³•å¾‹æ¡æ–‡åˆ†æ®µï¼Œæ£€æŸ¥å…¶æ˜¯å¦å…·æœ‰è¯­ä¹‰å®Œæ•´æ€§ï¼š

åˆ†æ®µå†…å®¹ï¼š
{segment.content}

å½“å‰å…ƒæ•°æ®ï¼š
- æ³•å¾‹: {segment.law_name}
- æ¡: {segment.article}
- ç« : {segment.chapter}

è¯·å›ç­”ï¼š
1. è¿™ä¸ªåˆ†æ®µæ˜¯å¦è¯­ä¹‰å®Œæ•´ï¼Ÿ
2. æ˜¯å¦éœ€è¦è°ƒæ•´åˆ†æ®µè¾¹ç•Œï¼Ÿ
3. å…ƒæ•°æ®æ˜¯å¦å‡†ç¡®ï¼Ÿ

è¿”å›æ ¼å¼ï¼š
{{
    "is_complete": true/false,
    "suggestions": "æ”¹è¿›å»ºè®®",
    "corrected_metadata": {{"ä¿®æ­£çš„å…ƒæ•°æ®"}}
}}
"""
        
        try:
            ai_response = self.call_deepseek_api(prompt)
            json_match = re.search(r'\{.*\}', ai_response, re.DOTALL)
            
            if json_match:
                analysis = json.loads(json_match.group())
                
                # æ ¹æ®AIå»ºè®®æ›´æ–°å…ƒæ•°æ®
                if "corrected_metadata" in analysis:
                    corrected = analysis["corrected_metadata"]
                    if corrected.get("article"):
                        segment.article = corrected["article"]
                    if corrected.get("chapter"):
                        segment.chapter = corrected["chapter"]
                
        except Exception as e:
            print(f"âš ï¸  åˆ†æ®µä¼˜åŒ–å¤±è´¥: {e}")
        
        return segment
    
    def extract_key_concepts(self, segment: LegalSegment) -> List[str]:
        """æå–åˆ†æ®µä¸­çš„å…³é”®æ³•å¾‹æ¦‚å¿µ"""
        if not self.api_key:
            return []
        
        prompt = f"""
è¯·ä»ä»¥ä¸‹æ³•å¾‹æ¡æ–‡ä¸­æå–å…³é”®çš„æ³•å¾‹æ¦‚å¿µå’Œæœ¯è¯­ï¼š

æ¡æ–‡å†…å®¹ï¼š
{segment.content}

è¯·è¿”å›JSONæ ¼å¼çš„å…³é”®æ¦‚å¿µåˆ—è¡¨ï¼š
{{"concepts": ["æ¦‚å¿µ1", "æ¦‚å¿µ2", "æ¦‚å¿µ3"]}}
"""
        
        try:
            ai_response = self.call_deepseek_api(prompt)
            json_match = re.search(r'\{.*\}', ai_response, re.DOTALL)
            
            if json_match:
                result = json.loads(json_match.group())
                return result.get("concepts", [])
                
        except Exception as e:
            print(f"âš ï¸  æ¦‚å¿µæå–å¤±è´¥: {e}")
        
        return []
    
    def generate_embeddings(self, segments: List[LegalSegment]) -> List[Dict[str, Any]]:
        """ä¸ºåˆ†æ®µç”Ÿæˆå‘é‡åµŒå…¥ï¼ˆæ¨¡æ‹Ÿï¼‰"""
        results = []
        
        for segment in segments:
            # å®é™…é¡¹ç›®ä¸­è¿™é‡Œä¼šè°ƒç”¨DeepSeekçš„embedding API
            # ç°åœ¨æˆ‘ä»¬æ¨¡æ‹Ÿè¿”å›ç»“æ„
            
            # æå–å…³é”®æ¦‚å¿µ
            key_concepts = self.extract_key_concepts(segment)
            
            result = {
                "segment_id": f"{segment.law_name}_{segment.article}",
                "content": segment.content,
                "metadata": segment.to_dict()["metadata"],
                "key_concepts": key_concepts,
                "embedding": [0.1] * 512,  # æ¨¡æ‹Ÿ512ç»´å‘é‡
                "embedding_model": "deepseek-embedding",
                "token_count": segment.token_count
            }
            
            results.append(result)
        
        return results
    
    def process_legal_document(
        self, 
        text: str, 
        law_name: str,
        use_ai_enhancement: bool = True
    ) -> Dict[str, Any]:
        """å®Œæ•´çš„æ³•å¾‹æ–‡æ¡£å¤„ç†æµç¨‹"""
        
        print(f"ğŸš€ å¼€å§‹å¤„ç†æ³•å¾‹æ–‡æ¡£: {law_name}")
        
        # 1. AIç»“æ„åˆ†æï¼ˆå¯é€‰ï¼‰
        structure_analysis = None
        if use_ai_enhancement and self.api_key:
            print("ğŸ¤– æ­£åœ¨è¿›è¡ŒAIç»“æ„åˆ†æ...")
            structure_analysis = self.ai_enhance_structure_recognition(text)
            print(f"âœ… ç»“æ„åˆ†æå®Œæˆ: {structure_analysis['document_type']}")
        
        # 2. åŸºç¡€åˆ†æ®µ
        print("ğŸ“ æ­£åœ¨è¿›è¡ŒåŸºç¡€åˆ†æ®µ...")
        segments = self.base_segmenter.segment_by_legal_structure(text, law_name)
        print(f"âœ… åŸºç¡€åˆ†æ®µå®Œæˆï¼Œå…± {len(segments)} ä¸ªåˆ†æ®µ")
        
        # 3. AIä¼˜åŒ–åˆ†æ®µï¼ˆå¯é€‰ï¼‰
        if use_ai_enhancement and self.api_key:
            print("ğŸ”§ æ­£åœ¨è¿›è¡ŒAIåˆ†æ®µä¼˜åŒ–...")
            segments = self.ai_optimize_segmentation(segments)
            print("âœ… åˆ†æ®µä¼˜åŒ–å®Œæˆ")
        
        # 4. ç”Ÿæˆå‘é‡åµŒå…¥
        print("âš¡ æ­£åœ¨ç”Ÿæˆå‘é‡åµŒå…¥...")
        embeddings = self.generate_embeddings(segments)
        print(f"âœ… å‘é‡ç”Ÿæˆå®Œæˆï¼Œå…± {len(embeddings)} ä¸ªå‘é‡")
        
        # 5. è¿”å›å®Œæ•´ç»“æœ
        result = {
            "law_name": law_name,
            "structure_analysis": structure_analysis,
            "segments": [segment.to_dict() for segment in segments],
            "embeddings": embeddings,
            "statistics": {
                "total_segments": len(segments),
                "total_tokens": sum(segment.token_count for segment in segments),
                "avg_tokens_per_segment": sum(segment.token_count for segment in segments) / len(segments) if segments else 0
            }
        }
        
        print("ğŸ‰ æ–‡æ¡£å¤„ç†å®Œæˆï¼")
        return result

def main():
    """æ¼”ç¤ºAIå¢å¼ºåŠŸèƒ½"""
    print("ğŸ¤– AIå¢å¼ºæ³•å¾‹æ–‡æ¡£å¤„ç†æ¼”ç¤º")
    print("=" * 60)
    
    # ç¤ºä¾‹æ–‡æ¡£
    sample_text = """
ä¸­åäººæ°‘å…±å’Œå›½åˆåŒæ³•

ç¬¬ä¸€ç«  ä¸€èˆ¬è§„å®š

ç¬¬ä¸€æ¡ ä¸ºäº†ä¿æŠ¤åˆåŒå½“äº‹äººçš„åˆæ³•æƒç›Šï¼Œç»´æŠ¤ç¤¾ä¼šç»æµç§©åºï¼Œä¿ƒè¿›ç¤¾ä¼šä¸»ä¹‰ç°ä»£åŒ–å»ºè®¾ï¼Œåˆ¶å®šæœ¬æ³•ã€‚

ç¬¬äºŒæ¡ æœ¬æ³•æ‰€ç§°åˆåŒæ˜¯å¹³ç­‰ä¸»ä½“çš„è‡ªç„¶äººã€æ³•äººã€å…¶ä»–ç»„ç»‡ä¹‹é—´è®¾ç«‹ã€å˜æ›´ã€ç»ˆæ­¢æ°‘äº‹æƒåˆ©ä¹‰åŠ¡å…³ç³»çš„åè®®ã€‚
å©šå§»ã€æ”¶å…»ã€ç›‘æŠ¤ç­‰æœ‰å…³èº«ä»½å…³ç³»çš„åè®®ï¼Œé€‚ç”¨å…¶ä»–æ³•å¾‹çš„è§„å®šã€‚

ç¬¬ä¸‰æ¡ åˆåŒå½“äº‹äººçš„æ³•å¾‹åœ°ä½å¹³ç­‰ï¼Œä¸€æ–¹ä¸å¾—å°†è‡ªå·±çš„æ„å¿—å¼ºåŠ ç»™å¦ä¸€æ–¹ã€‚

ç¬¬å››æ¡ å½“äº‹äººä¾æ³•äº«æœ‰è‡ªæ„¿è®¢ç«‹åˆåŒçš„æƒåˆ©ï¼Œä»»ä½•å•ä½å’Œä¸ªäººä¸å¾—éæ³•å¹²é¢„ã€‚
"""
    
    # åˆ›å»ºAIå¢å¼ºå¤„ç†å™¨ï¼ˆæ³¨æ„ï¼šéœ€è¦è®¾ç½®DEEPSEEK_API_KEYç¯å¢ƒå˜é‡ï¼‰
    processor = AIEnhancedLegalProcessor()
    
    # å¤„ç†æ–‡æ¡£
    result = processor.process_legal_document(
        sample_text, 
        "ä¸­åäººæ°‘å…±å’Œå›½åˆåŒæ³•",
        use_ai_enhancement=False  # è®¾ä¸ºFalseé¿å…APIè°ƒç”¨
    )
    
    # æ˜¾ç¤ºç»“æœ
    print("\nğŸ“Š å¤„ç†ç»“æœç»Ÿè®¡:")
    stats = result["statistics"]
    print(f"ğŸ“‹ æ€»åˆ†æ®µæ•°: {stats['total_segments']}")
    print(f"âš¡ æ€»Tokenæ•°: {stats['total_tokens']}")
    print(f"ğŸ“Š å¹³å‡Tokenæ•°: {stats['avg_tokens_per_segment']:.1f}")
    
    print("\nğŸ“ åˆ†æ®µé¢„è§ˆ:")
    for i, segment_data in enumerate(result["segments"][:3], 1):
        metadata = segment_data["metadata"]
        content_preview = segment_data["content"][:100] + "..."
        print(f"\n{i}. ğŸ“ {metadata['law_name']} > {metadata['chapter']} > ç¬¬{metadata['article']}æ¡")
        print(f"   ğŸ’¬ {content_preview}")

if __name__ == "__main__":
    main() 