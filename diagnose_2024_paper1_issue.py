#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
2024å¹´å·ä¸€å®Œæ•´ç‰ˆæ–‡æ¡£é—®é¢˜è¯Šæ–­è„šæœ¬
ä¸“é—¨è¯Šæ–­ä¸ºä»€ä¹ˆåªèƒ½è¯»å–åˆ°4ä¸ªé¢˜ç›®çš„é—®é¢˜
"""

import os
import re
from docx import Document
from docx.oxml.text.paragraph import CT_P
from docx.oxml.table import CT_Tbl
from docx.text.paragraph import Paragraph
from docx.table import Table, _Cell
import json

def analyze_document_structure(file_path):
    """æ·±åº¦åˆ†ææ–‡æ¡£ç»“æ„"""
    print(f"\nğŸ” æ­£åœ¨åˆ†ææ–‡æ¡£: {file_path}")
    print("=" * 60)
    
    if not os.path.exists(file_path):
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        return None
    
    try:
        doc = Document(file_path)
        print(f"âœ… æ–‡æ¡£åŠ è½½æˆåŠŸ")
        
        analysis = {
            'file_info': {
                'path': file_path,
                'size': os.path.getsize(file_path),
                'exists': True
            },
            'document_structure': {
                'total_paragraphs': len(doc.paragraphs),
                'total_tables': len(doc.tables),
                'sections': len(doc.sections)
            },
            'content_analysis': {
                'question_patterns': [],
                'paragraph_samples': [],
                'potential_questions': [],
                'all_question_numbers': [],
                'problematic_patterns': []
            }
        }
        
        print(f"ğŸ“Š åŸºæœ¬ä¿¡æ¯:")
        print(f"   - æ®µè½æ€»æ•°: {len(doc.paragraphs)}")
        print(f"   - è¡¨æ ¼æ€»æ•°: {len(doc.tables)}")
        print(f"   - èŠ‚æ•°: {len(doc.sections)}")
        print(f"   - æ–‡ä»¶å¤§å°: {os.path.getsize(file_path):,} å­—èŠ‚")
        
        # åˆ†ææ®µè½å†…å®¹
        print(f"\nğŸ“ æ®µè½å†…å®¹åˆ†æ:")
        question_count = 0
        question_numbers = []
        
        for i, para in enumerate(doc.paragraphs[:50]):  # åˆ†æå‰50ä¸ªæ®µè½
            text = para.text.strip()
            if not text:
                continue
                
            # æ£€æŸ¥æ˜¯å¦æ˜¯é¢˜ç›®
            question_match = re.match(r'^(\d+)[ï¼.\s]*(.+)', text)
            if question_match:
                q_num = int(question_match.group(1))
                if 1 <= q_num <= 100:  # é¢˜ç›®ç¼–å·èŒƒå›´
                    question_count += 1
                    question_numbers.append(q_num)
                    analysis['content_analysis']['potential_questions'].append({
                        'paragraph_index': i,
                        'question_number': q_num,
                        'text_preview': text[:100] + "..." if len(text) > 100 else text,
                        'full_text_length': len(text)
                    })
                    
                    if question_count <= 10:  # æ˜¾ç¤ºå‰10é¢˜
                        print(f"   é¢˜ç›® {q_num}: {text[:80]}...")
            
            # è®°å½•æ®µè½æ ·æœ¬
            if i < 20:  # å‰20ä¸ªæ®µè½æ ·æœ¬
                analysis['content_analysis']['paragraph_samples'].append({
                    'index': i,
                    'text_preview': text[:100] + "..." if len(text) > 100 else text,
                    'length': len(text),
                    'is_empty': len(text) == 0
                })
        
        analysis['content_analysis']['all_question_numbers'] = sorted(question_numbers)
        
        print(f"\nğŸ¯ é¢˜ç›®è¯†åˆ«ç»“æœ:")
        print(f"   - é€šè¿‡åŸºç¡€æ¨¡å¼è¯†åˆ«çš„é¢˜ç›®æ•°: {question_count}")
        print(f"   - é¢˜ç›®ç¼–å·èŒƒå›´: {min(question_numbers) if question_numbers else 'N/A'} - {max(question_numbers) if question_numbers else 'N/A'}")
        
        if question_numbers:
            missing_numbers = []
            for i in range(1, 101):
                if i not in question_numbers:
                    missing_numbers.append(i)
            
            if missing_numbers:
                print(f"   - ç¼ºå¤±çš„é¢˜ç›®ç¼–å·: {missing_numbers[:20]}{'...' if len(missing_numbers) > 20 else ''}")
                analysis['content_analysis']['missing_numbers'] = missing_numbers
        
        # æ·±åº¦æ–‡æœ¬æ¨¡å¼åˆ†æ
        print(f"\nğŸ”¬ æ·±åº¦æ¨¡å¼åˆ†æ:")
        
        all_text = "\n".join([p.text for p in doc.paragraphs if p.text.strip()])
        
        # å„ç§é¢˜ç›®æ¨¡å¼
        patterns = [
            (r'\d+[ï¼.]\s*[^0-9]', 'æ ‡å‡†é¢˜ç›®æ¨¡å¼ (æ•°å­—+ç‚¹+å†…å®¹)'),
            (r'\d+\s*[ï¼.]\s*[^0-9]', 'å¸¦ç©ºæ ¼é¢˜ç›®æ¨¡å¼'),
            (r'^\d+[ï¼.]', 'è¡Œé¦–æ•°å­—æ¨¡å¼'),
            (r'ç¬¬\d+é¢˜', 'ç¬¬Xé¢˜æ¨¡å¼'),
            (r'\d+ã€', 'æ•°å­—é¡¿å·æ¨¡å¼'),
            (r'\n\d+\s', 'æ¢è¡Œæ•°å­—æ¨¡å¼')
        ]
        
        for pattern, desc in patterns:
            matches = re.findall(pattern, all_text, re.MULTILINE)
            print(f"   - {desc}: æ‰¾åˆ° {len(matches)} ä¸ªåŒ¹é…")
            if matches:
                analysis['content_analysis']['question_patterns'].append({
                    'pattern': pattern,
                    'description': desc,
                    'match_count': len(matches),
                    'samples': matches[:5]
                })
        
        # æ£€æŸ¥ç‰¹æ®Šå­—ç¬¦å’Œç¼–ç é—®é¢˜
        print(f"\nğŸ”¤ å­—ç¬¦ç¼–ç æ£€æŸ¥:")
        special_chars = set()
        for para in doc.paragraphs:
            for char in para.text:
                if ord(char) > 127:  # éASCIIå­—ç¬¦
                    special_chars.add(char)
        
        common_special = ['ï¼', 'ã€‚', 'ï¼Œ', 'ã€', 'ï¼š', 'ï¼›', 'ï¼Ÿ', 'ï¼']
        found_special = [char for char in common_special if char in special_chars]
        print(f"   - å‘ç°çš„ç‰¹æ®Šæ ‡ç‚¹: {found_special}")
        
        # æ£€æŸ¥æ–‡æ¡£æ ¼å¼é—®é¢˜
        print(f"\nğŸ“‹ æ–‡æ¡£æ ¼å¼æ£€æŸ¥:")
        
        # æ£€æŸ¥è¡¨æ ¼å†…å®¹
        if doc.tables:
            print(f"   - å‘ç° {len(doc.tables)} ä¸ªè¡¨æ ¼ï¼Œå¯èƒ½é¢˜ç›®åœ¨è¡¨æ ¼ä¸­")
            for i, table in enumerate(doc.tables[:3]):  # æ£€æŸ¥å‰3ä¸ªè¡¨æ ¼
                print(f"     è¡¨æ ¼ {i+1}: {len(table.rows)} è¡Œ x {len(table.columns)} åˆ—")
                for row_idx, row in enumerate(table.rows[:5]):  # æ¯ä¸ªè¡¨æ ¼çœ‹å‰5è¡Œ
                    row_text = " | ".join([cell.text.strip() for cell in row.cells])
                    if row_text.strip():
                        print(f"       è¡Œ{row_idx+1}: {row_text[:100]}...")
        
        # ä¿å­˜åˆ†æç»“æœ
        output_file = file_path.replace('.docx', '_diagnostic_report.json')
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(analysis, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ è¯¦ç»†åˆ†ææŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_file}")
        return analysis
        
    except Exception as e:
        print(f"âŒ åˆ†æè¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")
        import traceback
        traceback.print_exc()
        return None

def test_different_parsing_methods(file_path):
    """æµ‹è¯•ä¸åŒçš„è§£ææ–¹æ³•"""
    print(f"\nğŸ§ª æµ‹è¯•ä¸åŒè§£ææ–¹æ³•:")
    print("=" * 40)
    
    try:
        doc = Document(file_path)
        
        # æ–¹æ³•1: ç›´æ¥æ®µè½è§£æ
        print("æ–¹æ³•1: ç›´æ¥æ®µè½è§£æ")
        method1_questions = []
        for para in doc.paragraphs:
            text = para.text.strip()
            if re.match(r'^\d+[ï¼.\s]', text):
                method1_questions.append(text[:50] + "...")
        print(f"   æ‰¾åˆ° {len(method1_questions)} ä¸ªå¯èƒ½çš„é¢˜ç›®")
        
        # æ–¹æ³•2: åŒ…å«è¡¨æ ¼çš„è§£æ
        print("\næ–¹æ³•2: åŒ…å«è¡¨æ ¼çš„è§£æ")
        method2_questions = []
        
        # æ®µè½
        for para in doc.paragraphs:
            text = para.text.strip()
            if re.match(r'^\d+[ï¼.\s]', text):
                method2_questions.append(text[:50] + "...")
        
        # è¡¨æ ¼
        for table in doc.tables:
            for row in table.rows:
                for cell in row.cells:
                    for para in cell.paragraphs:
                        text = para.text.strip()
                        if re.match(r'^\d+[ï¼.\s]', text):
                            method2_questions.append(f"[è¡¨æ ¼]{text[:50]}...")
        
        print(f"   æ‰¾åˆ° {len(method2_questions)} ä¸ªå¯èƒ½çš„é¢˜ç›®")
        
        # æ–¹æ³•3: æ­£åˆ™è¡¨è¾¾å¼æ‰«æå…¨æ–‡
        print("\næ–¹æ³•3: æ­£åˆ™è¡¨è¾¾å¼æ‰«æå…¨æ–‡")
        all_text = ""
        for para in doc.paragraphs:
            all_text += para.text + "\n"
        for table in doc.tables:
            for row in table.rows:
                for cell in row.cells:
                    all_text += cell.text + "\n"
        
        # å¤šç§æ­£åˆ™æ¨¡å¼
        patterns = [
            r'(\d+)[ï¼.]\s*([^0-9\n]{10,})',  # æ ‡å‡†æ¨¡å¼
            r'(\d+)\s*[ï¼.]\s*([^0-9\n]{10,})',  # å¸¦ç©ºæ ¼
            r'^(\d+)[ï¼.]\s*(.+)$',  # è¡Œé¦–
        ]
        
        method3_questions = []
        for pattern in patterns:
            matches = re.findall(pattern, all_text, re.MULTILINE)
            for match in matches:
                if len(match) >= 2:
                    q_num = int(match[0])
                    if 1 <= q_num <= 100:
                        method3_questions.append(f"{q_num}. {match[1][:50]}...")
        
        method3_questions = list(set(method3_questions))  # å»é‡
        print(f"   æ‰¾åˆ° {len(method3_questions)} ä¸ªå¯èƒ½çš„é¢˜ç›®")
        
        # æ˜¾ç¤ºæ¯ç§æ–¹æ³•çš„å‰å‡ ä¸ªç»“æœ
        for i, (method_name, questions) in enumerate([
            ("æ–¹æ³•1", method1_questions),
            ("æ–¹æ³•2", method2_questions), 
            ("æ–¹æ³•3", method3_questions)
        ], 1):
            print(f"\n{method_name}çš„å‰5ä¸ªç»“æœ:")
            for j, q in enumerate(questions[:5]):
                print(f"   {j+1}. {q}")
        
        return {
            'method1': len(method1_questions),
            'method2': len(method2_questions),
            'method3': len(method3_questions)
        }
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")
        return None

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ”§ 2024å¹´å·ä¸€å®Œæ•´ç‰ˆæ–‡æ¡£é—®é¢˜è¯Šæ–­å·¥å…·")
    print("=" * 60)
    
    file_path = "/Users/acheng/Downloads/law-exam-assistant/2024å¹´å·ä¸€å®Œæ•´ç‰ˆ_ä¿®å¤ç‰ˆ.docx"
    
    # 1. åŸºç¡€æ–‡æ¡£åˆ†æ
    analysis = analyze_document_structure(file_path)
    
    # 2. æµ‹è¯•ä¸åŒè§£ææ–¹æ³•
    parsing_results = test_different_parsing_methods(file_path)
    
    # 3. æ€»ç»“å’Œå»ºè®®
    print(f"\nğŸ“‹ è¯Šæ–­æ€»ç»“:")
    print("=" * 40)
    
    if analysis and analysis['content_analysis']['potential_questions']:
        total_found = len(analysis['content_analysis']['potential_questions'])
        print(f"âœ… åŸºç¡€åˆ†ææ‰¾åˆ° {total_found} ä¸ªé¢˜ç›®")
        
        if total_found < 100:
            print(f"âš ï¸  é¢˜ç›®æ•°é‡ä¸è¶³ï¼Œåº”è¯¥æœ‰100é¢˜")
            print(f"ğŸ’¡ å¯èƒ½çš„åŸå› :")
            print(f"   - æ–‡æ¡£æ ¼å¼ä¸é¢„æœŸä¸ç¬¦")
            print(f"   - é¢˜ç›®å­˜å‚¨åœ¨è¡¨æ ¼ä¸­")
            print(f"   - ä½¿ç”¨äº†ç‰¹æ®Šçš„ç¼–å·æ ¼å¼")
            print(f"   - è¯»å–è„šæœ¬çš„æ­£åˆ™è¡¨è¾¾å¼éœ€è¦è°ƒæ•´")
    else:
        print(f"âŒ åŸºç¡€åˆ†ææœªæ‰¾åˆ°é¢˜ç›®ï¼Œè¿™å¾ˆä¸æ­£å¸¸")
    
    if parsing_results:
        print(f"\nğŸ“Š ä¸åŒè§£ææ–¹æ³•ç»“æœå¯¹æ¯”:")
        for method, count in parsing_results.items():
            print(f"   {method}: {count} ä¸ªé¢˜ç›®")
    
    print(f"\nğŸ”§ å»ºè®®çš„è§£å†³æ­¥éª¤:")
    print(f"1. æ£€æŸ¥è¯¦ç»†åˆ†ææŠ¥å‘Š: 2024å¹´å·ä¸€å®Œæ•´ç‰ˆ_ä¿®å¤ç‰ˆ_diagnostic_report.json")
    print(f"2. æ ¹æ®æŠ¥å‘Šè°ƒæ•´è¯»å–è„šæœ¬çš„æ­£åˆ™è¡¨è¾¾å¼")
    print(f"3. å¦‚æœé¢˜ç›®åœ¨è¡¨æ ¼ä¸­ï¼Œéœ€è¦ä¿®æ”¹è„šæœ¬ä»¥è§£æè¡¨æ ¼å†…å®¹")
    print(f"4. è€ƒè™‘ä½¿ç”¨æ›´å®½æ¾çš„é¢˜ç›®è¯†åˆ«æ¨¡å¼")

if __name__ == "__main__":
    main()