#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
向量化和向量数据库存储模块
用于将文本块转换为向量嵌入，并存储到向量数据库中
支持：
1. DeepSeek API生成向量嵌入
2. MySQL/SQLite存储向量数据
3. 向量检索功能
"""

import os
import json
import time
import numpy as np
import requests
from typing import List, Dict, Any, Union, Optional
import logging
import sqlite3
import pickle
import base64

# 可选：如果安装了faiss，则导入用于高效向量检索
try:
    import faiss
    FAISS_AVAILABLE = True
except ImportError:
    FAISS_AVAILABLE = False
    print("FAISS库未安装，将使用原生向量检索方法")

# 可选：如果安装了MySQL连接器，则导入
try:
    import mysql.connector
    from mysql.connector import Error as MySQLError
    MYSQL_AVAILABLE = True
except ImportError:
    MYSQL_AVAILABLE = False
    print("MySQL连接器未安装，将使用SQLite作为备用")
    MySQLError = Exception  # 定义一个占位符

# 配置日志
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger('vector_store')

class VectorStore:
    """向量存储类，处理向量化和数据库存储"""
    
    def __init__(
        self,
        database_type: str = "sqlite",
        mysql_config: Dict[str, Any] = None,
        sqlite_path: str = "data/vector_store.db",
        api_key: str = None,
        embedding_model: str = "text-embedding-ada-002"
    ):
        """
        初始化向量存储
        
        Args:
            database_type: 数据库类型，支持'mysql'或'sqlite'
            mysql_config: MySQL连接配置
            sqlite_path: SQLite数据库文件路径
            api_key: DeepSeek API密钥，如果为None则尝试从环境变量DEEPSEEK_API_KEY获取
            embedding_model: 使用的向量嵌入模型
        """
        self.database_type = database_type
        self.mysql_config = mysql_config
        self.sqlite_path = sqlite_path
        
        # 优先使用传入的API密钥，否则尝试从环境变量获取
        self.api_key = api_key or os.environ.get('DEEPSEEK_API_KEY')
        if not self.api_key:
            logger.warning("未设置DeepSeek API密钥，向量嵌入功能将不可用。请设置DEEPSEEK_API_KEY环境变量或直接传递api_key参数。")
        
        self.embedding_model = embedding_model
        self.vector_dim = 1536  # DeepSeek默认向量维度
        self.index = None  # FAISS索引
        self.doc_ids = []  # 文档ID列表
        
        # 确保数据库已初始化
        self.init_database()
        
        # 确保SQLite数据库目录存在
        if self.database_type == 'sqlite':
            os.makedirs(os.path.dirname(sqlite_path), exist_ok=True)
    
    def init_database(self):
        """初始化数据库表结构"""
        if self.database_type == 'mysql' and MYSQL_AVAILABLE:
            try:
                conn = mysql.connector.connect(**self.mysql_config)
                cursor = conn.cursor()
                
                # 创建向量存储表
                cursor.execute('''
                CREATE TABLE IF NOT EXISTS vector_chunks (
                    id INT AUTO_INCREMENT PRIMARY KEY,
                    doc_id VARCHAR(50) NOT NULL,
                    chunk_index INT NOT NULL,
                    original_text TEXT NOT NULL,
                    vector_embedding LONGBLOB NOT NULL,
                    source_document_name VARCHAR(100) NOT NULL,
                    chunk_id_in_document VARCHAR(50) NOT NULL,
                    law_name VARCHAR(100),
                    book VARCHAR(50),
                    chapter VARCHAR(100),
                    section VARCHAR(100),
                    article VARCHAR(50),
                    token_count INT,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    INDEX (doc_id, chunk_index),
                    INDEX (source_document_name),
                    INDEX (law_name),
                    INDEX (chapter),
                    INDEX (article)
                ) DEFAULT CHARSET=utf8mb4
                ''')
                
                conn.commit()
                cursor.close()
                conn.close()
                logger.info("MySQL向量存储表初始化成功")
                
            except Exception as e:
                logger.error(f"MySQL初始化失败: {str(e)}")
                if self.database_type == 'mysql':
                    logger.info("切换到SQLite备用数据库")
                    self.database_type = 'sqlite'
                    self.init_database()
        
        if self.database_type == 'sqlite':
            try:
                conn = sqlite3.connect(self.sqlite_path)
                cursor = conn.cursor()
                
                # 创建向量存储表
                cursor.execute('''
                CREATE TABLE IF NOT EXISTS vector_chunks (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    doc_id TEXT NOT NULL,
                    chunk_index INTEGER NOT NULL,
                    original_text TEXT NOT NULL,
                    vector_embedding BLOB NOT NULL,
                    source_document_name TEXT NOT NULL,
                    chunk_id_in_document TEXT NOT NULL,
                    law_name TEXT,
                    book TEXT,
                    chapter TEXT,
                    section TEXT,
                    article TEXT,
                    token_count INTEGER,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
                ''')
                
                # 创建索引
                cursor.execute('CREATE INDEX IF NOT EXISTS idx_doc_chunk ON vector_chunks(doc_id, chunk_index)')
                cursor.execute('CREATE INDEX IF NOT EXISTS idx_source ON vector_chunks(source_document_name)')
                cursor.execute('CREATE INDEX IF NOT EXISTS idx_law ON vector_chunks(law_name)')
                cursor.execute('CREATE INDEX IF NOT EXISTS idx_article ON vector_chunks(article)')
                
                conn.commit()
                cursor.close()
                conn.close()
                logger.info("SQLite向量存储表初始化成功")
                
            except Exception as e:
                logger.error(f"SQLite初始化失败: {str(e)}")
    
    def get_embedding_from_deepseek(self, text: str) -> List[float]:
        """
        调用DeepSeek API获取文本的向量嵌入
        
        Args:
            text: 要转换为向量的文本
            
        Returns:
            向量嵌入列表
        """
        if not self.api_key:
            raise ValueError("未设置DeepSeek API密钥，无法生成向量嵌入")
        
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {self.api_key}"
        }
        
        # DeepSeek API URL - 更新为正确的API端点
        url = "https://api-embed.deepseek.com/v1/embeddings"
        
        # 请求参数 - 使用正确的模型名称
        payload = {
            "input": text,
            "model": "deepseek-embed"  # 更新为DeepSeek的嵌入模型
        }
        
        try:
            response = requests.post(url, headers=headers, json=payload)
            response.raise_for_status()
            
            result = response.json()
            embedding = result['data'][0]['embedding']
            return embedding
            
        except requests.exceptions.RequestException as e:
            logger.error(f"DeepSeek API请求失败: {str(e)}")
            if response := getattr(e, "response", None):
                logger.error(f"API响应: {response.status_code} - {response.text}")
            raise
    
    def batch_get_embeddings(self, texts: List[str], batch_size: int = 10) -> List[List[float]]:
        """
        批量获取多个文本的向量嵌入
        
        Args:
            texts: 文本列表
            batch_size: 批处理大小，避免API请求过于频繁
            
        Returns:
            向量嵌入列表的列表
        """
        embeddings = []
        total_batches = (len(texts) + batch_size - 1) // batch_size
        
        # 如果文本量很大，记录开始时间并输出信息
        start_time = time.time()
        if len(texts) > 50:
            logger.info(f"开始处理大量文本嵌入: {len(texts)}个文本，分{total_batches}个批次")
        
        for i in range(0, len(texts), batch_size):
            batch = texts[i:i + batch_size]
            batch_num = i//batch_size + 1
            logger.info(f"处理批次 {batch_num}/{total_batches}，包含{len(batch)}个文本")
            
            # 计算已处理百分比和预计剩余时间
            if batch_num > 1 and len(texts) > 50:
                elapsed = time.time() - start_time
                progress = batch_num / total_batches
                estimated_total = elapsed / progress
                remaining = estimated_total - elapsed
                logger.info(f"进度: {progress*100:.1f}%, 已用时: {elapsed:.1f}秒, 预计剩余: {remaining:.1f}秒")
            
            try:
                # 尝试批量API调用 (如果API支持)
                if False:  # 替换为实际的批量API支持检查
                    # 批量API调用代码
                    pass
                else:
                    # 逐个处理批次中的文本
                    for text in batch:
                        try:
                            embedding = self.get_embedding_from_deepseek(text)
                            embeddings.append(embedding)
                        except Exception as e:
                            logger.error(f"获取文本嵌入失败: {str(e)}")
                            # 添加一个零向量作为占位符
                            embeddings.append([0.0] * self.vector_dim)
                
                # 批次间添加小延迟，避免API速率限制
                if batch_num < total_batches:
                    time.sleep(0.2)
            except Exception as e:
                logger.error(f"批次处理失败: {str(e)}")
                # 为批次中的所有文本添加零向量
                embeddings.extend([[0.0] * self.vector_dim] * len(batch))
        
        # 记录完成情况
        if len(texts) > 50:
            total_time = time.time() - start_time
            logger.info(f"向量嵌入处理完成，总耗时: {total_time:.1f}秒，平均每个文本: {total_time/len(texts):.3f}秒")
        
        return embeddings
    
    def vector_to_binary(self, vector: List[float]) -> bytes:
        """将向量列表转换为二进制数据，用于数据库存储"""
        return pickle.dumps(np.array(vector, dtype=np.float32))
    
    def binary_to_vector(self, binary_data: bytes) -> List[float]:
        """将二进制数据转换回向量列表"""
        return pickle.loads(binary_data).tolist()
    
    def store_text_chunks(
        self, 
        doc_id: str, 
        texts: List[str], 
        metadata_list: List[Dict[str, Any]],
        source_document_name: str
    ) -> bool:
        """
        存储文本块及其向量嵌入到数据库
        
        Args:
            doc_id: 文档ID
            texts: 文本块列表
            metadata_list: 元数据列表，与texts一一对应
            source_document_name: 源文档名称
            
        Returns:
            是否成功存储
        """
        if len(texts) != len(metadata_list):
            logger.error("文本块数量与元数据数量不匹配")
            return False
        
        try:
            # 如果处理的是大量文本，记录开始时间
            start_time = time.time()
            is_large_batch = len(texts) > 50
            
            if is_large_batch:
                logger.info(f"开始为{len(texts)}个文本块生成向量嵌入和数据库存储")
            
            # 生成向量嵌入
            logger.info(f"为{len(texts)}个文本块生成向量嵌入")
            embeddings = self.batch_get_embeddings(texts)
            
            if len(embeddings) != len(texts):
                logger.error(f"生成的向量数量({len(embeddings)})与文本数量({len(texts)})不匹配")
                return False
            
            # 准备要插入的数据
            insert_data = []
            for i, (text, embedding, meta) in enumerate(zip(texts, embeddings, metadata_list)):
                chunk_id = f"{doc_id}_{i}"
                vector_binary = self.vector_to_binary(embedding)
                    
            insert_data.append((
                    doc_id,
                    i,
                    text,
                    vector_binary,
                    source_document_name,
                    chunk_id,
                    meta.get('law', '未知法律'),
                    meta.get('编', '未识别'),
                    meta.get('章', '未识别'),
                    meta.get('节', '未识别'),
                    meta.get('条', '未识别'),
                    meta.get('token_count', 0)
                    ))
                
            # 以分批方式存储到数据库，避免一次性提交太多数据
            batch_size = 100  # 每批写入100条记录
            total_batches = (len(insert_data) + batch_size - 1) // batch_size
            
            if is_large_batch:
                logger.info(f"开始写入数据库，共{len(insert_data)}条记录，分{total_batches}批处理")
            
            # 存储到数据库
            if self.database_type == 'mysql' and MYSQL_AVAILABLE:
                # 建立一次性连接，使用事务进行批量处理
                conn = mysql.connector.connect(**self.mysql_config)
                cursor = conn.cursor()
                
                try:
                    # 开始事务
                    conn.start_transaction()
                    
                    # 插入查询
                    insert_query = """
                    INSERT INTO vector_chunks (
                        doc_id, chunk_index, original_text, vector_embedding, 
                        source_document_name, chunk_id_in_document, law_name,
                        book, chapter, section, article, token_count
                    ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                    """
                    
                    # 分批执行插入
                    for batch_idx in range(total_batches):
                        start_idx = batch_idx * batch_size
                        end_idx = min(start_idx + batch_size, len(insert_data))
                        batch_data = insert_data[start_idx:end_idx]
                        
                        cursor.executemany(insert_query, batch_data)
                        
                        if is_large_batch:
                            progress = (batch_idx + 1) / total_batches
                            logger.info(f"数据库写入进度: {progress*100:.1f}% ({batch_idx+1}/{total_batches}批)")
                    
                    # 提交事务
                    conn.commit()
                    
                except Exception as e:
                    # 回滚事务
                    conn.rollback()
                    logger.error(f"MySQL写入失败，已回滚: {str(e)}")
                    raise
                
                finally:
                    # 关闭连接
                    cursor.close()
                    conn.close()
            
            if self.database_type == 'sqlite':
                # 开始事务 (SQLite默认是自动提交的，这里显式开始事务)
                cursor.execute("BEGIN TRANSACTION")
                
                # 插入查询
                insert_query = """
                INSERT INTO vector_chunks (
                    doc_id, chunk_index, original_text, vector_embedding, 
                    source_document_name, chunk_id_in_document, law_name,
                    book, chapter, section, article, token_count
                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                """
                
                # 分批执行插入
                for batch_idx in range(total_batches):
                    start_idx = batch_idx * batch_size
                    end_idx = min(start_idx + batch_size, len(insert_data))
                    batch_data = insert_data[start_idx:end_idx]
                    
                    cursor.executemany(insert_query, batch_data)
                    
                    if is_large_batch:
                        progress = (batch_idx + 1) / total_batches
                        logger.info(f"数据库写入进度: {progress*100:.1f}% ({batch_idx+1}/{total_batches}批)")
                
                # 提交事务
                cursor.execute("COMMIT")
            
            if is_large_batch:
                total_time = time.time() - start_time
                logger.info(f"成功存储{len(texts)}个文本块到{self.database_type}数据库，总耗时: {total_time:.1f}秒")
            else:
                logger.info(f"成功存储{len(texts)}个文本块到{self.database_type}数据库")
                
            return True
            
        except Exception as e:
            logger.error(f"存储文本块失败: {str(e)}")
            return False
    
    def load_vectors_for_search(self) -> bool:
        """
        从数据库加载向量到内存中，用于搜索
        默认只加载最近添加的10万条记录，避免内存压力
        
        Returns:
            是否成功加载
        """
        try:
            vectors = []
            doc_ids = []
            
            if self.database_type == 'mysql' and MYSQL_AVAILABLE:
                conn = mysql.connector.connect(**self.mysql_config)
                cursor = conn.cursor(dictionary=True)
                
                # 只加载最近的10万条记录
                query = """
                SELECT id, chunk_id_in_document, vector_embedding
                FROM vector_chunks
                ORDER BY created_at DESC
                LIMIT 100000
                """
                
                cursor.execute(query)
                results = cursor.fetchall()
                
                for row in results:
                    vector = self.binary_to_vector(row['vector_embedding'])
                    vectors.append(vector)
                    doc_ids.append(row['chunk_id_in_document'])
                
                cursor.close()
                conn.close()
                
            else:
                # 使用SQLite
                conn = sqlite3.connect(self.sqlite_path)
                conn.row_factory = sqlite3.Row
                cursor = conn.cursor()
                
                # 只加载最近的10万条记录
                query = """
                SELECT id, chunk_id_in_document, vector_embedding
                FROM vector_chunks
                ORDER BY created_at DESC
                LIMIT 100000
                """
                
                cursor.execute(query)
                results = cursor.fetchall()
                
                for row in results:
                    vector = self.binary_to_vector(row['vector_embedding'])
                    vectors.append(vector)
                    doc_ids.append(row['chunk_id_in_document'])
                
                cursor.close()
                conn.close()
            
            # 创建向量索引
            if FAISS_AVAILABLE and vectors:
                vectors_np = np.array(vectors, dtype=np.float32)
                self.vector_dim = vectors_np.shape[1]
                self.index = faiss.IndexFlatL2(self.vector_dim)
                self.index.add(vectors_np)
                self.doc_ids = doc_ids
                logger.info(f"成功加载{len(vectors)}个向量到FAISS索引")
            elif vectors:
                self.index = np.array(vectors, dtype=np.float32)
                self.doc_ids = doc_ids
                logger.info(f"成功加载{len(vectors)}个向量到NumPy数组")
            else:
                logger.warning("没有找到向量数据")
                return False
            
            return True
            
        except Exception as e:
            logger.error(f"加载向量失败: {str(e)}")
            return False
    
    def search_similar_texts(
        self,
        query_text: str,
        top_k: int = 5,
        filters: Dict[str, Any] = None
    ) -> List[Dict[str, Any]]:
        """
        搜索与查询文本相似的文档
        
        Args:
            query_text: 查询文本
            top_k: 返回最相似的文档数量
            filters: 过滤条件，例如 {'law_name': '中华人民共和国宪法'}
            
        Returns:
            相似文档列表
        """
        try:
            # 检查API密钥是否已设置
            if not self.api_key:
                logger.error("DeepSeek API密钥未设置，无法执行向量搜索")
                return []
            
            # 生成查询文本的向量嵌入
            try:
                query_vector = self.get_embedding_from_deepseek(query_text)
            except Exception as e:
                logger.error(f"生成查询向量失败: {str(e)}")
                return []
            
            # 如果向量索引未加载，则加载
            if self.index is None:
                if not self.load_vectors_for_search():
                    logger.error("无法加载向量索引")
                    return []
            
            # 如果索引不可用，返回空结果
            if self.index is None:
                logger.error("向量索引不可用，无法搜索")
                return []
            
            # 执行向量相似性搜索
            if FAISS_AVAILABLE:
                query_np = np.array([query_vector], dtype=np.float32)
                distances, indices = self.index.search(query_np, min(top_k * 3, len(self.doc_ids)))
                
                # 获取搜索结果ID
                chunk_ids = [self.doc_ids[i] for i in indices[0]]
            else:
                # 如果没有FAISS，使用NumPy计算余弦相似度
                query_np = np.array(query_vector, dtype=np.float32)
                
                # 计算余弦相似度 (点积 / (norm1 * norm2))
                norm_query = np.linalg.norm(query_np)
                norm_docs = np.linalg.norm(self.index, axis=1)
                
                # 避免除零错误
                safe_norm_docs = np.where(norm_docs > 0, norm_docs, 1e-10)
                
                # 计算点积
                dot_products = np.dot(self.index, query_np)
                
                # 计算余弦相似度
                similarities = dot_products / (norm_query * safe_norm_docs)
                
                # 获取相似度最高的索引
                indices = np.argsort(similarities)[::-1][:top_k * 3]
                
                # 获取搜索结果ID
                chunk_ids = [self.doc_ids[i] for i in indices]
            
            # 从数据库获取文档详情
            results = []
            
            if self.database_type == 'mysql' and MYSQL_AVAILABLE:
                conn = mysql.connector.connect(**self.mysql_config)
                cursor = conn.cursor(dictionary=True)
                
                # 构建WHERE子句
                where_clauses = ["chunk_id_in_document IN (%s)" % ','.join(['%s'] * len(chunk_ids))]
                params = chunk_ids.copy()
                
                # 添加过滤条件
                if filters:
                    for key, value in filters.items():
                        if value:
                            where_clauses.append(f"{key} = %s")
                            params.append(value)
                
                # 构建查询
                query = f"""
                SELECT * FROM vector_chunks
                WHERE {' AND '.join(where_clauses)}
                ORDER BY FIELD(chunk_id_in_document, {','.join(['%s'] * len(chunk_ids))})
                LIMIT %s
                """
                
                # 添加排序和限制参数
                params.extend(chunk_ids)
                params.append(top_k)
                
                cursor.execute(query, params)
                results = cursor.fetchall()
                
                # 转换binary字段
                for result in results:
                    if 'vector_embedding' in result:
                        del result['vector_embedding']  # 避免返回大型二进制数据
                
                cursor.close()
                conn.close()
                
            else:
                # 使用SQLite
                conn = sqlite3.connect(self.sqlite_path)
                conn.row_factory = sqlite3.Row
                cursor = conn.cursor()
                
                # 构建WHERE子句
                where_clauses = [f"chunk_id_in_document IN ({','.join(['?'] * len(chunk_ids))})"]
                params = chunk_ids.copy()
                
                # 添加过滤条件
                if filters:
                    for key, value in filters.items():
                        if value:
                            where_clauses.append(f"{key} = ?")
                            params.append(value)
                
                # 构建查询
                query = f"""
                SELECT * FROM vector_chunks
                WHERE {' AND '.join(where_clauses)}
                LIMIT ?
                """
                
                # 添加限制参数
                params.append(top_k)
                
                cursor.execute(query, params)
                results = [dict(row) for row in cursor.fetchall()]
                
                # 转换binary字段
                for result in results:
                    if 'vector_embedding' in result:
                        del result['vector_embedding']  # 避免返回大型二进制数据
                
                cursor.close()
                conn.close()
            
            logger.info(f"找到{len(results)}个相似文档")
            return results
            
        except Exception as e:
            logger.error(f"搜索文档失败: {str(e)}")
            return []

def main():
    """测试向量存储功能"""
    import argparse
    
    parser = argparse.ArgumentParser(description='向量存储测试工具')
    parser.add_argument('--db', choices=['mysql', 'sqlite'], default='sqlite', help='数据库类型')
    parser.add_argument('--api_key', help='DeepSeek API密钥，也可以通过环境变量DEEPSEEK_API_KEY设置')
    parser.add_argument('--text', help='要转换为向量的测试文本')
    parser.add_argument('--search', help='搜索相似文档的查询文本')
    
    args = parser.parse_args()
    
    # 创建向量存储实例
    vector_store = VectorStore(
        database_type=args.db,
        api_key=args.api_key
    )
    
    # 测试向量生成
    if args.text:
        try:
            text = args.text
            print(f"\n生成文本向量: {text[:50]}...")
            
            # 生成向量
            start_time = time.time()
            embedding = vector_store.get_embedding_from_deepseek(text)
            elapsed = time.time() - start_time
            
            print(f"生成向量成功! 维度: {len(embedding)}, 耗时: {elapsed:.2f}秒")
            print(f"向量片段: {embedding[:5]}... {embedding[-5:]}")
            
            # 存储测试
            print("\n存储向量测试...")
            doc_id = f"test_{int(time.time())}"
            meta = {'law': '测试法律', '章': '测试章节', '条': '测试条款', 'token_count': len(text)}
            
            success = vector_store.store_text_chunks(
                doc_id=doc_id,
                texts=[text],
                metadata_list=[meta],
                source_document_name="测试文档"
            )
            
            if success:
                print(f"向量存储成功! 文档ID: {doc_id}")
            else:
                print("向量存储失败")
            
        except Exception as e:
            print(f"向量生成失败: {str(e)}")
    
    # 测试向量搜索
    if args.search:
        try:
            query = args.search
            print(f"\n搜索相似文档: {query}")
            
            # 加载向量
            print("加载向量索引...")
            vector_store.load_vectors_for_search()
            
            # 搜索
            start_time = time.time()
            results = vector_store.search_similar_texts(query, top_k=3)
            elapsed = time.time() - start_time
            
            print(f"搜索完成! 找到{len(results)}个结果, 耗时: {elapsed:.2f}秒")
            
            # 显示结果
            for i, result in enumerate(results, 1):
                print(f"\n结果 {i}:")
                print(f"法律: {result.get('law_name')}")
                print(f"章节: {result.get('chapter')}")
                print(f"条款: {result.get('article')}")
                print(f"内容片段: {result.get('original_text')[:100]}...")
                
        except Exception as e:
            print(f"搜索失败: {str(e)}")

if __name__ == "__main__":
    main() 