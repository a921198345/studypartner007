#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
æµ‹è¯•æ³•å¾‹æ–‡æ¡£æ™ºèƒ½åˆ†æ®µåŠŸèƒ½
"""

import os
import sys
import json
from text_segmentation import LegalDocumentSegmenter, LegalSegment

def test_legal_segmentation():
    """æµ‹è¯•æ³•å¾‹æ–‡æ¡£åˆ†æ®µåŠŸèƒ½"""
    
    # æ¨¡æ‹Ÿçš„æ³•å¾‹æ–‡æ¡£å†…å®¹
    sample_legal_text = """
ä¸­åäººæ°‘å…±å’Œå›½æ°‘æ³•å…¸

ç¬¬ä¸€ç¼– æ€»åˆ™

ç¬¬ä¸€ç«  åŸºæœ¬è§„å®š

ç¬¬ä¸€æ¡ ä¸ºäº†ä¿æŠ¤æ°‘äº‹ä¸»ä½“çš„åˆæ³•æƒç›Šï¼Œè°ƒæ•´æ°‘äº‹å…³ç³»ï¼Œç»´æŠ¤ç¤¾ä¼šå’Œç»æµç§©åºï¼Œé€‚åº”ä¸­å›½ç‰¹è‰²ç¤¾ä¼šä¸»ä¹‰å‘å±•è¦æ±‚ï¼Œå¼˜æ‰¬ç¤¾ä¼šä¸»ä¹‰æ ¸å¿ƒä»·å€¼è§‚ï¼Œæ ¹æ®å®ªæ³•ï¼Œåˆ¶å®šæœ¬æ³•ã€‚

ç¬¬äºŒæ¡ æ°‘æ³•è°ƒæ•´å¹³ç­‰ä¸»ä½“çš„è‡ªç„¶äººã€æ³•äººå’Œéæ³•äººç»„ç»‡ä¹‹é—´çš„äººèº«å…³ç³»å’Œè´¢äº§å…³ç³»ã€‚

ç¬¬ä¸‰æ¡ æ°‘äº‹ä¸»ä½“çš„äººèº«æƒåˆ©ã€è´¢äº§æƒåˆ©ä»¥åŠå…¶ä»–åˆæ³•æƒç›Šå—æ³•å¾‹ä¿æŠ¤ï¼Œä»»ä½•ç»„ç»‡æˆ–è€…ä¸ªäººä¸å¾—ä¾µçŠ¯ã€‚

ç¬¬äºŒç«  è‡ªç„¶äºº

ç¬¬ä¸€èŠ‚ æ°‘äº‹æƒåˆ©èƒ½åŠ›å’Œæ°‘äº‹è¡Œä¸ºèƒ½åŠ›

ç¬¬åæ¡ è‡ªç„¶äººçš„æ°‘äº‹æƒåˆ©èƒ½åŠ›ä¸€å¾‹å¹³ç­‰ã€‚

ç¬¬åä¸€æ¡ åå…«å‘¨å²ä»¥ä¸Šçš„è‡ªç„¶äººä¸ºæˆå¹´äººã€‚ä¸æ»¡åå…«å‘¨å²çš„è‡ªç„¶äººä¸ºæœªæˆå¹´äººã€‚

ç¬¬åäºŒæ¡ ä¸æ»¡å…«å‘¨å²çš„æœªæˆå¹´äººä¸ºæ— æ°‘äº‹è¡Œä¸ºèƒ½åŠ›äººï¼Œç”±å…¶æ³•å®šä»£ç†äººä»£ç†å®æ–½æ°‘äº‹æ³•å¾‹è¡Œä¸ºã€‚

ç¬¬äºŒèŠ‚ ç›‘æŠ¤

ç¬¬äºŒåå…­æ¡ çˆ¶æ¯å¯¹æœªæˆå¹´å­å¥³è´Ÿæœ‰æŠšå…»ã€æ•™è‚²å’Œä¿æŠ¤çš„ä¹‰åŠ¡ã€‚
ï¼ˆä¸€ï¼‰æŠšå…»ä¹‰åŠ¡åŒ…æ‹¬æä¾›ç”Ÿæ´»è´¹ç”¨ã€åŒ»ç–—è´¹ç”¨ç­‰ã€‚
ï¼ˆäºŒï¼‰æ•™è‚²ä¹‰åŠ¡åŒ…æ‹¬æ¥å—ä¹‰åŠ¡æ•™è‚²ã€é“å¾·æ•™è‚²ç­‰ã€‚
1. ç¡®ä¿æ¥å—ä¹å¹´ä¹‰åŠ¡æ•™è‚²ã€‚
2. è¿›è¡Œå¿…è¦çš„é“å¾·å“æ ¼æ•™è‚²ã€‚

ç¬¬äºŒåä¸ƒæ¡ çˆ¶æ¯æ˜¯æœªæˆå¹´å­å¥³çš„ç›‘æŠ¤äººã€‚
"""
    
    print("ğŸš€ å¼€å§‹æµ‹è¯•æ³•å¾‹æ–‡æ¡£æ™ºèƒ½åˆ†æ®µåŠŸèƒ½")
    print("=" * 60)
    
    # åˆ›å»ºåˆ†æ®µå™¨
    segmenter = LegalDocumentSegmenter()
    
    # è¿›è¡Œåˆ†æ®µ
    print("ğŸ“ æ­£åœ¨åˆ†ææ³•å¾‹æ–‡æ¡£ç»“æ„...")
    segments = segmenter.segment_by_legal_structure(
        sample_legal_text, 
        "ä¸­åäººæ°‘å…±å’Œå›½æ°‘æ³•å…¸",
        max_tokens=500,  # è¾ƒå°çš„å€¼ä¾¿äºæµ‹è¯•
        min_tokens=50
    )
    
    print(f"âœ… åˆ†æ®µå®Œæˆï¼å…±ç”Ÿæˆ {len(segments)} ä¸ªåˆ†æ®µ")
    print("=" * 60)
    
    # æ˜¾ç¤ºåˆ†æ®µç»“æœ
    for i, segment in enumerate(segments, 1):
        print(f"\nğŸ“‹ **åˆ†æ®µ {i}**")
        print("-" * 40)
        print(f"ğŸ›ï¸  **æ³•å¾‹åç§°**: {segment.law_name}")
        print(f"ğŸ“š  **ç¼–**: {segment.book or 'æ— '}")
        print(f"ğŸ“–  **ç« **: {segment.chapter or 'æ— '}")
        print(f"ğŸ“‘  **èŠ‚**: {segment.section or 'æ— '}")
        print(f"ğŸ“ƒ  **æ¡**: {segment.article or 'æ— '}")
        print(f"ğŸ“  **æ¬¾**: {segment.paragraph or 'æ— '}")
        print(f"ğŸ”¢  **é¡¹**: {segment.item or 'æ— '}")
        print(f"âš¡  **Tokenæ•°é‡**: {segment.token_count}")
        print(f"ğŸ“„  **å†…å®¹é•¿åº¦**: {len(segment.content)} å­—ç¬¦")
        
        # æ˜¾ç¤ºå†…å®¹é¢„è§ˆ
        content_preview = segment.content.replace('\n', ' ').strip()
        if len(content_preview) > 150:
            content_preview = content_preview[:150] + "..."
        print(f"ğŸ“  **å†…å®¹é¢„è§ˆ**: {content_preview}")
        
        # æ˜¾ç¤ºå®Œæ•´å…ƒæ•°æ®
        metadata = segment.to_dict()['metadata']
        print(f"ğŸ·ï¸  **å…ƒæ•°æ®**: {metadata}")
        print()
    
    # ç»Ÿè®¡ä¿¡æ¯
    print("=" * 60)
    print("ğŸ“Š **åˆ†æ®µç»Ÿè®¡ä¿¡æ¯**")
    print("-" * 40)
    
    total_tokens = sum(segment.token_count for segment in segments)
    avg_tokens = total_tokens / len(segments) if segments else 0
    
    books = set(segment.book for segment in segments if segment.book)
    chapters = set(segment.chapter for segment in segments if segment.chapter)
    sections = set(segment.section for segment in segments if segment.section)
    articles = set(segment.article for segment in segments if segment.article)
    
    print(f"ğŸ“Š æ€»åˆ†æ®µæ•°: {len(segments)}")
    print(f"âš¡ æ€»Tokenæ•°: {total_tokens}")
    print(f"ğŸ“Š å¹³å‡Tokenæ•°: {avg_tokens:.1f}")
    print(f"ğŸ“š æ¶‰åŠç¼–æ•°: {len(books)}")
    print(f"ğŸ“– æ¶‰åŠç« æ•°: {len(chapters)}")
    print(f"ğŸ“‘ æ¶‰åŠèŠ‚æ•°: {len(sections)}")
    print(f"ğŸ“ƒ æ¶‰åŠæ¡æ•°: {len(articles)}")
    
    return segments

def demo_search_capabilities(segments):
    """æ¼”ç¤ºåŸºäºç»“æ„åŒ–å…ƒæ•°æ®çš„æœç´¢èƒ½åŠ›"""
    print("\n" + "=" * 60)
    print("ğŸ” **æ¼”ç¤ºæœç´¢åŠŸèƒ½**")
    print("-" * 40)
    
    # æ¨¡æ‹Ÿæœç´¢åœºæ™¯
    search_queries = [
        ("ç¬¬ä¸€ç« çš„æ‰€æœ‰å†…å®¹", lambda s: s.chapter and "åŸºæœ¬è§„å®š" in s.chapter),
        ("å…³äºç›‘æŠ¤çš„æ¡æ–‡", lambda s: s.section and "ç›‘æŠ¤" in s.section),
        ("ç¬¬åä¸€æ¡çš„å†…å®¹", lambda s: s.article and s.article == "åä¸€"),
        ("æ°‘äº‹æƒåˆ©ç›¸å…³æ¡æ–‡", lambda s: "æ°‘äº‹æƒåˆ©" in s.content)
    ]
    
    for query_desc, search_func in search_queries:
        print(f"\nğŸ” æœç´¢: {query_desc}")
        results = [segment for segment in segments if search_func(segment)]
        
        if results:
            print(f"âœ… æ‰¾åˆ° {len(results)} ä¸ªç›¸å…³åˆ†æ®µ:")
            for i, result in enumerate(results, 1):
                location = []
                if result.book: location.append(f"ç¼–:{result.book}")
                if result.chapter: location.append(f"ç« :{result.chapter}")
                if result.section: location.append(f"èŠ‚:{result.section}")
                if result.article: location.append(f"æ¡:{result.article}")
                
                location_str = " > ".join(location) if location else "ä½ç½®ä¿¡æ¯ä¸å®Œæ•´"
                content_preview = result.content.replace('\n', ' ').strip()[:100] + "..."
                
                print(f"  {i}. ğŸ“ {location_str}")
                print(f"     ğŸ’¬ {content_preview}")
        else:
            print("âŒ æœªæ‰¾åˆ°ç›¸å…³å†…å®¹")

if __name__ == "__main__":
    # è¿è¡Œæµ‹è¯•
    segments = test_legal_segmentation()
    
    # æ¼”ç¤ºæœç´¢åŠŸèƒ½
    demo_search_capabilities(segments)
    
    print("\n" + "=" * 60)
    print("ğŸ‰ æµ‹è¯•å®Œæˆï¼")
    print("âœ¨ ä½ çš„æ³•å¾‹ç»“æ„åˆ†æ®µæ–¹æ¡ˆéå¸¸æ£’!")
    print("ğŸ“ˆ å»ºè®®ä¸‹ä¸€æ­¥: é›†æˆDeepSeek APIè¿›è¡Œå‘é‡åŒ–") 